# -*- coding: utf-8 -*-
"""Data 301 Heart Disease.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t0D7EzL2Lh7drG5XE8zLhOguFSeqx_qc

#INTRODUCTION

This database contains 76 attributes, but all published experiments refer to using a subset of 14 of them. In particular, the Cleveland database is the only one that has been used by ML researchers to this date. The "goal" field refers to the presence of heart disease in the patient. It is integer valued from 0 (no presence) to 4. Experiments with the Cleveland database have concentrated on simply attempting to distinguish presence (values 1,2,3,4) from absence (value 0).

DATASET COLUMNS FEATURE EXPLAIN

*   Age (age in years)
*   Sex (1 = male; 0 = female)
*CP (chest pain type)
*TRESTBPS (resting blood pressure (in mm Hg on admission to the hospital))
*CHOL (serum cholestoral in mg/dl)
FPS (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)
*RESTECH (resting electrocardiographic results)
*THALACH (maximum heart rate achieved)
EXANG (exercise induced angina (1 = yes; 0 = no))
*OLDPEAK (ST depression induced by exercise relative to rest)
*SLOPE (the slope of the peak exercise ST segment)
*CA (number of major vessels (0-3) colored by flourosopy)
*THAL (3 = normal; 6 = fixed defect; 7 = reversable defect)
*TARGET (1 or 0)
"""

import numpy as np
import pandas as pd

data=pd.read_csv('heart.csv')

data

data.describe()

"""Describe function is a function that allows analysis between the numerical values contained in the data set. Using this function count, mean, std, min, max, 25%, 50%, 75%.

As seen in this section, most values are generally categorized. This means that we need to integrate other values into this situation. These; age, trestbps, chol, thalach.
"""

data.info()

data.columns

data=data.rename(columns={'age':'Age','sex':'Sex','cp':'Cp','trestbps':'Trestbps','chol':'Chol','fbs':'Fbs','restecg':'Restecg','thalach':'Thalach','exang':'Exang','oldpeak':'Oldpeak','slope':'Slope','ca':'Ca','thal':'Thal','target':'Target'})

data.columns

#And, how many rows and columns are there for all data?
data.shape 
#first one is rows, other is columns

"""#Data Cleaning"""

import matplotlib.pyplot as plt
import seaborn as sns

#Now,I will check null on all data and If data has null, I will sum of null data's. In this way, how many missing data is in the data.
print('Data Sum of Null Values \n')
data.isnull().sum()

#all rows control for null values
data.isnull().values.any()

fig,ax=plt.subplots(figsize=(15,5))
sns.heatmap(data.isnull(), annot=True)

"""THAT'S Good. No Missing Values

##Correlation
"""

#plt.figure(figsize=(10,10))
#sns.heatmap(data.corr(),annot=True,fmt='.1f')
#plt.show()
fig=plt.figure(figsize=(10,10))
sns.heatmap(data.corr(), annot= True, cmap='Blues')

"""##Feature Analysis

How many people are suffering from Heart Disease ?
"""

fig,ax=plt.subplots(1, 2, figsize = (14,5))
sns.countplot(data=data, x='Target', ax=ax[0])
ax[0].set_xlabel("Disease Count \n [0]->No [1]->Yes")
ax[0].set_ylabel("Count")
ax[0].set_title("Heart Disease Count")
data['Target'].value_counts().plot.pie()
plt.title("Heart Disease")

"""From above graph we can say that more than half of the population suffering from Heart Disease with parcentage of 54.5%.

###Based on SEX
"""

fig,ax=plt.subplots(1,2,figsize=(14,5))
sns.countplot(x='Sex',data=data,hue='Target',palette='Set2',ax=ax[0])
ax[0].set_xlabel("0 ->Female , 1 ->Male")
data.Sex.value_counts().plot.pie(ax=ax[1],autopct='%1.1f%%',shadow=True, explode=[0.1,0], cmap='Reds')
ax[1].set_title("0 ->Female , 1 -> Male")

"""Number of Women suffering from Heart Disease are more than Men but Men population is more than Women. We will use these insights for our model development.

###Based on Fasting Blood Sugar (FBS)
"""

fig,ax=plt.subplots(1,2,figsize=(14,5))
sns.countplot(x='Fbs',data=data,hue='Target',palette='Set3',ax=ax[0])
ax[0].set_xlabel("0-> fbs <120 , 1-> fbs>120",size=12)
data.Fbs.value_counts().plot.pie(ax=ax[1],autopct='%1.1f%%',shadow=True, explode=[0.1,0],cmap='Oranges')
ax[1].set_title("0 -> fbs <120 , 1 -> fbs>120",size=12)

"""People having fps < 120 have more chance of having Heart Disease than people having fps >120

##Based on Rest ECG
"""

fig,ax=plt.subplots(1,2,figsize=(14,5))
sns.countplot(x='Restecg',data=data,hue='Target',palette='Set2',ax=ax[0])
ax[0].set_xlabel("Resting Electrocardiographic",size=12)
data.Restecg.value_counts().plot.pie(ax=ax[1],autopct='%1.1f%%',shadow=True,
                                     explode=[0.005,0.05,0.05],cmap='Blues')
ax[1].set_title("Resting Electrocardiographic",size=12)

"""if resting electrocardiographic is 1 then person have more chances of suffering from Heart Disease

#Based on peak excersise ST segment
"""

fig,ax=plt.subplots(1,2,figsize=(14,5))
sns.countplot(x='Slope',data=data,hue='Target',palette='Set3',ax=ax[0])
ax[0].set_xlabel("peak exercise ST segment",size=12)
data.Slope.value_counts().plot.pie(ax=ax[1],autopct='%1.1f%%',shadow=True,explode=[0.005,0.05,0.05],cmap='Blues')

ax[1].set_title("peak exercise ST segment ",size=12)

"""Feature (the peak exercise ST segment slope) has three symbolic values (flat, up sloping, downsloping)

Therefore People having up sloping are more prone to Heart Disease than flat and downsloping. This is useful for our model

##Analyzing heart disease over age distribution
"""

pd.crosstab(data.Age, data.Target).plot(kind='bar', figsize=(20, 10))
plt.title("People having heart disease vs people not having heart disease for a given age")
plt.xlabel("Age Distribution")
plt.ylabel("Heart Disease Frequency")

"""We could infer from the chart above that age is not a huge influencing factor for heart disease

##Analyzing heart disease over Maximum Heart Rate
"""

my_colors = 'yr'
pd.crosstab(data.Thalach, data.Target).plot(kind='bar', figsize=(20,10), color=my_colors)
plt.title("Heart diseases frequency for Maximum heart rate")
plt.xlabel("Maximum Heart Rate for a person")
plt.ylabel("Heart Disease Frequency")

"""We could infer from the above chart that people who have higher heart rate has a higher probability of having a heart disease

##Analyzing heart disease frequency over chest pain type

Chest pain type:

1 --> Typical angina

2 --> Atypical angina

3 --> Non-anginal pain

4 --> Asymptomatic
"""

pd.crosstab(data.Cp, data.Target).plot(kind='bar', figsize = (20,10))
plt.title("Heart disease frequency over Chesr Pain Type")
plt.xlabel("Cheast Pain Type ")
plt.ylabel("Heart Disease Frequency")

"""We could infer from the above chart that people who have Atypical angina or non-anginal pain have a higher probability of having heart disease

#NEURAL NETWORKS
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
# %matplotlib inline

a = pd.get_dummies(data['Cp'], prefix = "Cp")
b = pd.get_dummies(data['Thal'], prefix = "Thal")
c = pd.get_dummies(data['Slope'], prefix = "Slope")
frames = [data, a, b, c]
data = pd.concat(frames, axis = 1)

to_be_dropped = ['Cp', 'Thal', 'Slope']
data = data.drop(to_be_dropped, axis=1)
data.head()

data = (data - np.min(data)) / (np.max(data) - np.min(data)).values
attributes = data.drop(columns='Target')
targets = data['Target']


from sklearn.model_selection import train_test_split
trainAttr, testAttr, trainTarget, testTarget = train_test_split(attributes, targets, test_size = 0.2, random_state = 42)

from tensorflow.python.keras import models
from tensorflow.python.keras import layers

model = models.Sequential()
model.add(layers.Dense(16,activation='relu', input_shape=(trainAttr.shape[1],)))
model.add(layers.Dense(16,activation='relu'))
#model.add(layers.Dense(16, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))


model.compile(optimizer='rmsprop',
              loss='binary_crossentropy',
              metrics=['accuracy'])

history = model.fit(trainAttr, trainTarget, epochs=8, batch_size=16)

results = model.evaluate(testAttr, testTarget)
results

"""#Logistic Regression"""

import pandas as pd
import numpy as np
from sklearn import model_selection
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.pipeline import make_pipeline
from scipy.stats import zscore
from sklearn.metrics import average_precision_score
from sklearn.metrics import f1_score, accuracy_score, precision_recall_fscore_support, confusion_matrix

x = np.array(data.drop(['Target', 'Fbs', 'Chol'], 1))
y = np.array(data['Target'])

x_train, x_test, y_train, y_test = model_selection.train_test_split(x, y, stratify=y, random_state=42, test_size = 0.2)

poly_model = make_pipeline(LogisticRegression(fit_intercept=False))

poly_model.fit(x_train, y_train)
predict = poly_model.predict(x_test)
print(predict)
print(y_test)

error = (abs(predict - y_test)).sum() / len(y_test)
error

print(f1_score(y_test, predict))
print(accuracy_score(y_test, predict))
print(average_precision_score(y_test, predict))
print(precision_recall_fscore_support(y_test, predict, average='weighted'))
sns.heatmap(confusion_matrix(y_test, predict), annot= True, cmap='Reds')
plt.title('F1 Score = {}'.format(f1_score(y_test, predict)))

fig,ax=plt.subplots(figsize=(10,5))
precision,recall,threshold = precision_recall_curve(y_test, predict)
ax.plot(recall,precision,'g--')
ax.set_xlabel('Recall')
ax.set_ylabel('Precision')
ax.set_title("Average Precision Score : {}".format(average_precision_score(y_test, predict)))

"""#KNN"""

from sklearn.preprocessing import StandardScaler, MinMaxScaler

data_label=data['Target']
del data['Target']
data_label=pd.DataFrame(data_label)
data_label

data_scaled=MinMaxScaler().fit_transform(data)
data_scaled=pd.DataFrame(data=data_scaled, columns=data.columns)
data_scaled

def CrossVal(dataX,dataY,mode,cv=3):
    score=cross_val_score(mode,dataX , dataY, cv=cv, scoring='accuracy')
    return(np.mean(score))

def plotting(true,pred):
    fig,ax=plt.subplots(figsize=(10,5))
    precision,recall,threshold = precision_recall_curve(true,pred[:,1])
    ax.plot(recall,precision,'g--')
    ax.set_xlabel('Recall')
    ax.set_ylabel('Precision')
    ax.set_title("Average Precision Score : {}".format(average_precision_score(true,pred[:,1])))

from sklearn.neighbors import KNeighborsRegressor
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import average_precision_score
from sklearn.metrics import f1_score
from sklearn.metrics import confusion_matrix

Xtrain,Xtest,Ytrain,Ytest = train_test_split(data_scaled, data_label, test_size=0.20, random_state=217)

for i in range(1, 20):
  k=KNeighborsClassifier(algorithm='auto',n_neighbors= i)
  score_k=CrossVal(Xtrain, Ytrain.values.ravel(), k)
  print('{} : {}'.format(i, score_k))

k=KNeighborsClassifier(algorithm='auto',n_neighbors= 17)
score_k=CrossVal(Xtrain,Ytrain.values.ravel(),k)
k.fit(Xtrain,Ytrain)

print("Accuracy is : ",score_k)
plotting(Ytest,k.predict_proba(Xtest))
fig=plt.figure()
sns.heatmap(confusion_matrix(Ytest,k.predict(Xtest)), annot= True, cmap='Reds')
k_f1=f1_score(Ytest,k.predict(Xtest))
plt.title('F1 Score = {}'.format(k_f1))

"""Accuracy of KNN -> 85%

Accuracy of Logical Regression -> 83%

Accuracy of Neural Networks -> 82-88%

#Results
"""

model_accuracy = pd.Series(data=[accuracy_score(y_test, predict), 0.8525, score_k], index = ['Logistic Regression', 'Neural Network', 'K-Nearest Neighbors'])

fig = plt.figure(figsize=(9,5))
model_accuracy.sort_values().plot.barh()
plt.title('Model Accuracy Score Comparison')

model_f1 = pd.Series(data=[f1_score(y_test, predict), k_f1], index = ['Logistic Regression', 'K-Nearest Neighbors'])

fig = plt.figure(figsize=(9,5))
model_f1.sort_values().plot.barh()
plt.title('Model F1 Score Comparison')

model_precision = pd.Series(data=[average_precision_score(y_test, predict), average_precision_score(Ytest,k.predict_proba(Xtest)[:,1])], index = ['Logistic Regression', 'K-Nearest Neighbors'])

fig = plt.figure(figsize=(9,5))
model_precision.sort_values().plot.barh()
plt.title('Model Precision Score Comparison')